{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNadeRo7s24ZPI8BUnN0pkV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehraj-alom/pytorch/blob/main/classification_evaluation_metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# classification evaluation metrics"
      ],
      "metadata": {
        "id": "Ow9EpErqlW1e"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3772ebeb"
      },
      "source": [
        "\n",
        "These are some of the most common methods you'll come across and are a good starting point.\n",
        "\n",
        "However, you may want to evaluate your classification model using more metrics such as the following:\n",
        "\n",
        "| Metric name/Evaluation method | Defintion | Code |\n",
        "|---|---|---|\n",
        "| Accuracy | Out of 100 predictions, how many does your model get correct? E.g. 95% accuracy means it gets 95/100 predictions correct. | `torchmetrics.Accuracy()` or `sklearn.metrics.accuracy_score()` |\n",
        "| Precision | Proportion of true positives over total number of samples. Higher precision leads to less false positives (model predicts 1 when it should've been 0). | `torchmetrics.Precision()` or `sklearn.metrics.precision_score()` |\n",
        "| Recall | Proportion of true positives over total number of true positives and false negatives (model predicts 0 when it should've been 1). Higher recall leads to less false negatives. | `torchmetrics.Recall()` or `sklearn.metrics.recall_score()` |\n",
        "| F1-score | Combines precision and recall into one metric. 1 is best, 0 is worst. | `torchmetrics.F1Score()` or `sklearn.metrics.f1_score()` |\n",
        "| Confusion matrix | Compares the predicted values with the true values in a tabular way, if 100% correct, all values in the matrix will be top left to bottom right (diagnol line). | `torchmetrics.ConfusionMatrix` or `sklearn.metrics.plot_confusion_matrix()` |\n",
        "| Classification report | Collection of some of the main classification metrics such as precision, recall and f1-score. | `sklearn.metrics.classification_report()` |\n",
        "\n",
        "Scikit-Learn (a popular and world-class machine learning library) has many implementations of the above metrics and you're looking for a PyTorch-like version, check out TorchMetrics, especially the TorchMetrics classification section."
      ]
    }
  ]
}