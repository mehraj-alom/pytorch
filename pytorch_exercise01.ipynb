{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPw0apuM3yZLSfGj5pmkYmI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehraj-alom/pytorch/blob/main/pytorch_exercise01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Exercises**\n",
        "\n",
        "All exercises have been inspired from code throughout the notebook.\n",
        "\n",
        "There is one exercise per major section.\n",
        "\n",
        "You should be able to complete them by referencing their specific section.\n",
        "\n",
        "Note: For all exercises, your code should be device agnostic (meaning it could run on CPU or GPU if it's available).\n",
        "\n",
        "Create a straight line dataset using the linear regression formula (weight * X + bias).\n",
        "Set weight=0.3 and bias=0.9 there should be at least 100 datapoints total.\n",
        "Split the data into 80% training, 20% testing.\n",
        "\n",
        "Plot the training and testing data so it becomes visual.\n",
        "\n",
        "Build a PyTorch model by subclassing nn.Module.\n",
        "\n",
        "Inside should be a randomly initialized nn.Parameter() with requires_grad=True, one for weights and one for bias.\n",
        "\n",
        "Implement the forward() method to compute the linear regression function you used to create the dataset in 1.\n",
        "\n",
        "Once you've constructed the model, make an instance of it and check its state_dict().\n",
        "\n",
        "Note: If you'd like to use nn.Linear() instead of nn.Parameter() you can.\n",
        "Create a loss function and optimizer using nn.L1Loss() and torch.optim.SGD(params, lr) respectively.\n",
        "\n",
        "Set the learning rate of the optimizer to be 0.01 and the parameters to optimize should be the model parameters from the model you created in 2.\n",
        "Write a training loop to perform the appropriate training steps for 300 epochs.\n",
        "The training loop should test the model on the test dataset every 20 epochs.\n",
        "Make predictions with the trained model on the test data.\n",
        "\n",
        "Visualize these predictions against the original training and testing data (note: you may need to make sure the predictions are not on the GPU if you want to use non-CUDA-enabled libraries such as matplotlib to plot).\n",
        "\n",
        "Save your trained model's state_dict() to file.\n",
        "\n",
        "Create a new instance of your model class you made in 2. and load in the state_dict() you just saved to it.\n",
        "\n",
        "Perform predictions on your test data with the loaded model and confirm they match the original model predictions from 4.\n",
        "Resource: See the exercises notebooks templates and solutions on the course GitHub.\n",
        "\n",
        "**Extra-curriculum**\n",
        "\n",
        "Listen to The Unofficial PyTorch Optimization Loop Song (to help remember the steps in a PyTorch training/testing loop).\n",
        "\n",
        "Read What is torch.nn, really? by Jeremy Howard for a deeper understanding of how one of the most important modules in PyTorch works.\n",
        "\n",
        "Spend 10-minutes scrolling through and checking out the PyTorch documentation cheatsheet for all of the different PyTorch modules you might come across.\n",
        "Spend 10-minutes reading the loading and saving documentation on the PyTorch website to become more familiar with the different saving and loading options in PyTorch.\n",
        "\n",
        "Spend 1-2 hours reading/watching the following for an overview of the internals of gradient descent and backpropagation, the two main algorithms that have been working in the background to help our model learn.\n",
        "\n",
        "Wikipedia page for gradient descent\n",
        "Gradient Descent Algorithm â€” a deep dive by Robert Kwiatkowski\n",
        "Gradient descent, how neural networks learn video by 3Blue1Brown\n",
        "What is backpropagation really doing? video by 3Blue1Brown\n",
        "Backpropagation Wikipedia Page"
      ],
      "metadata": {
        "id": "pxRSDtkS6YqT"
      }
    }
  ]
}